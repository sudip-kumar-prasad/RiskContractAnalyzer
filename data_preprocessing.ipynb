{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing Pipeline\n",
        "This notebook contains the complete data preprocessing code to check step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Document Loader Code\n",
        "Run this cell to define the code that loads `.txt` or `.pdf` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "\n",
        "def load_text_from_file(file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Reads text data from a PDF or TXT file.\n",
        "    Args:\n",
        "        file_path (str): The path to the file.\n",
        "    Returns:\n",
        "        str: The extracted text.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"The file {file_path} was not found.\")\n",
        "\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    file_extension = file_extension.lower()\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            return f.read()\n",
        "    \n",
        "    elif file_extension == '.pdf':\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                for page in reader.pages:\n",
        "                    extracted = page.extract_text()\n",
        "                    if extracted:\n",
        "                        text += extracted + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF {file_path}: {e}\")\n",
        "        return text\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file format: {file_extension}. Only .txt and .pdf are supported.\")\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Text Cleaner Code\n",
        "Run this cell to define the text cleaning functions (NLTK downloads, lowercasing, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Setup NLTK resources\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    \n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "# Ensure punkt_tab is downloaded if needed by newer nltk versions\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans the input text by:\n",
        "    - Lowercasing\n",
        "    - Removing special characters, punctuation, and extra whitespace\n",
        "    - Removing stopwords\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    \n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove special characters and punctuation (keep alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    \n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
        "    \n",
        "    # Join tokens back into a single string\n",
        "    cleaned_text = ' '.join(cleaned_tokens)\n",
        "    \n",
        "    # Remove extra spaces that might be left\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    \n",
        "    return cleaned_text\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Segmenter Code\n",
        "Run this cell to define the clause segmentation logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import List\n",
        "\n",
        "def segment_into_clauses(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Segments a full contract text into individual clauses.\n",
        "    This uses a basic heuristic approach to split by common clause delimiters\n",
        "    like numbered lists (1., 2.), bullet points, or double newlines.\n",
        "\n",
        "    Args:\n",
        "        text (str): The full raw text of the contract.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of segmented clauses.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Strategy 1: Split by double newlines (paragraphs)\n",
        "    # Often, distinct clauses are separated by empty lines.\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "    \n",
        "    clauses = []\n",
        "    \n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "            \n",
        "        # Strategy 2: Further split by numbering (e.g., \"1.\", \"1.1\", \"a)\") if they appear inside a paragraph\n",
        "        # This regex looks for a newline/start followed by numbers/letters and a dot or parenthesis\n",
        "        # Example: \"\\n 1. \" or \"^a) \"\n",
        "        # We replace these delimiters with a special marker to split easily\n",
        "        \n",
        "        # This is a simple regex for demonstration. Complex legal documents might need more robust parsing.\n",
        "        delimiters = r'(?m)(^\\s*\\d+\\.\\d*\\s*|^\\s*[a-z]\\)\\s*|^\\s*[ivx]+\\.\\s*)'\n",
        "        \n",
        "        # Split the paragraph by these delimiters\n",
        "        parts = re.split(delimiters, para)\n",
        "        \n",
        "        # Reconstruct the clauses\n",
        "        current_clause = \"\"\n",
        "        for i, part in enumerate(parts):\n",
        "            if re.match(delimiters, part):\n",
        "                # If we have an existing clause, save it before starting a new one\n",
        "                if current_clause:\n",
        "                    clauses.append(current_clause.strip())\n",
        "                current_clause = part # Start new clause with its delimiter\n",
        "            else:\n",
        "                current_clause += part\n",
        "                \n",
        "        if current_clause:\n",
        "            clauses.append(current_clause.strip())\n",
        "\n",
        "    # Filter out very short strings that are likely not real clauses\n",
        "    final_clauses = [c for c in clauses if len(c.split()) > 3]\n",
        "    \n",
        "    return final_clauses\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Demo Execution\n",
        "This cell executes the pipeline on the sample contract data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filepath = 'data/sample_contract.txt'\n",
        "\n",
        "print(f\"Loading document from: {filepath}...\")\n",
        "raw_text = load_text_from_file(filepath)\n",
        "print(f\"\\nDocument loaded successfully. Extracted {len(raw_text)} characters.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Segmenting into clauses...\")\n",
        "clauses = segment_into_clauses(raw_text)\n",
        "print(f\"Found {len(clauses)} potential clauses.\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"Preprocessing top 5 clauses:\")\n",
        "for i, clause in enumerate(clauses[:5], 1):\n",
        "    cleaned = clean_text(clause)\n",
        "    print(f\"\\nClause {i} (Raw):\")\n",
        "    print(f\"  {clause[:150]}\")\n",
        "    print(f\"Clause {i} (Cleaned for ML):\")\n",
        "    print(f\"  {cleaned[:150]}\")\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}